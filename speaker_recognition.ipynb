{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systemy Multimedialne\n",
    "System identyfikacji mówcy wykorzystujący konwolucyjne sieci neuronowe\n",
    "\n",
    "<div>\n",
    "<img src=\"assets/SM_schemat_przetwarzania.png\" height=\"600\"/>\n",
    "</div>\n",
    "\n",
    "### Biblioteki\n",
    "- `torch`, `torchaudio`, `torchvision` - biblioteki do uczenia maszynowego\n",
    "- `librosa` - biblioteka do przetwarzania muzyki i dźwięku\n",
    "- `numpy` - biblioteka do przetwarzania wielowymiarowych macierzy\n",
    "- `matplotlib` - biblioteka do tworzenia wykresów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_ROOT = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przetwarzanie wstępne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ekstrakcja cech - tworzenie spektogramu melowego\n",
    "Na potrzeby sieci neuronowej, wstępnie przetworzone pliki z mową, zostały przetworzone na spektogramy melowe w rozmiarze 224 x 224 x 3.\n",
    "\n",
    "Pliki dźwiękowe zostały podzielone na ramki z wykorzystaniem **funkcji okienkującej Hamminga**. Następnie za pomocą transformaty Fouriera, ramki zostały przetworzone na wstępne spektogramy. Przed podaniem ich do sieci, spektogramy zostały zlogratymowane oraz zmapowane do skali melowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametry ekstrakcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 44100\n",
    "N_FFT = 2048\n",
    "FRAME_SIZE = 2048\n",
    "HOP_SIZE = 512\n",
    "N_MELS = 128\n",
    "WINDOW_FN = torch.hamming_window\n",
    "POWER = 2\n",
    "PAD_MODE = 'reflect'\n",
    "NORM = 'slaney'\n",
    "MEL_SCALE = 'htk'\n",
    "RESAMPLING_METHOD = 'kaiser_best'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mel_spectrogram = T.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=N_FFT,\n",
    "    win_length=FRAME_SIZE,\n",
    "    hop_length=HOP_SIZE,\n",
    "    n_mels=N_MELS,\n",
    "    window_fn=WINDOW_FN,\n",
    "    power=POWER,\n",
    "    pad_mode=PAD_MODE,\n",
    "    norm=NORM,\n",
    "    mel_scale=MEL_SCALE,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_spectrogram(sg, title, ylabel):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title)\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel(\"frame\")\n",
    "    im = axs.imshow(librosa.power_to_db(sg), origin=\"lower\", aspect=\"auto\")\n",
    "    fig.colorbar(im, ax=axs)\n",
    "    plt.show(block=False)\n",
    "\n",
    "def plot_waveform(waveform, sr, title=\"Waveform\"):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sr\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    axes.plot(time_axis, waveform[0], linewidth=1)\n",
    "    axes.grid(True)\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Załadowanie pliku dźwiękowego do wektora dźwiękowego o określonej częstotliwości, z wykorzystaniem funkcji . Funkcja `load` zwraca wektor i częstotliwość próbki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_file = \"audio/test.wav\"\n",
    "SPEECH_WAVEFORM, SAMPLE_RATE = torchaudio.load(test_file)\n",
    "plot_waveform(SPEECH_WAVEFORM, SAMPLE_RATE, title=\"Original waveform\")\n",
    "ipd.Audio(SPEECH_WAVEFORM, rate=SAMPLE_RATE)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "melspec = mel_spectrogram(SPEECH_WAVEFORM)\n",
    "print(melspec.shape)\n",
    "plot_spectrogram(melspec[0], title=\"MelSpectrogram - torchaudio\", ylabel=\"mel freq\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Przygotowanie danych"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N_SPEAKERS = 5\n",
    "CLASSES = N_SPEAKERS + 2\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-7\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.1220, 0.1220, 0.1220], [0.2058, 0.2058, 0.2058])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.1220, 0.1220, 0.1220], [0.2058, 0.2058, 0.2058])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.1220, 0.1220, 0.1220], [0.2058, 0.2058, 0.2058])\n",
    "    ]),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_ROOT, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "data_loaders = {x: torch.utils.data.DataLoader(image_datasets[x],\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=4)\n",
    "               for x in ['train', 'val', 'test']}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATASET_SIZES = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "CLASS_NAMES = image_datasets['train'].classes\n",
    "print(CLASS_NAMES)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Przygotowanie sieci konwolucyjnej\n",
    "### Transfer wiedzy\n",
    "W systemie skorzystano z metody transferu wiedzy, która polega na użyciu sieci neuronowej wytrenowanej wcześniej w innym, podobnym celu.\n",
    "\n",
    "Istnieją dwa główne podejścia do transferu wiedzy. Pierwszy polega na zainicjowaniu sieci z wykorzystaniem wstępnie wytrenowanych wag zamiast inicjalizacji losowej i dalszym trenowaniu z wykorzystaniem docelowych danych. Drugie podejście polega na adaptacji wstępnie wyszkolonej sieci jako ekstraktora cech, w którym tylko ostatnia, w pełni połączona warstwa jest losowa inicjowana i trenowana.\n",
    "\n",
    "W systemie zastosowano drugie podejście i wykorzystano sieć SqueezeNet, wytrenowaną w celu rozpoznawania obiektów za pomocą zbioru danych ImageNet."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_1', weights=models.SqueezeNet1_1_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier._modules[\"1\"] = nn.Sequential(\n",
    "    nn.Conv2d(512, CLASSES, kernel_size=(1, 1)),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "model.num_classes = CLASSES\n",
    "\n",
    "m_squeezenet = model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parametry uczenia"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Uczenie"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler=None, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    losses = {'train': [],\n",
    "              'val': []}\n",
    "    accs = {'train': [],\n",
    "              'val': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in data_loaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train' and scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / DATASET_SIZES[phase]\n",
    "            epoch_acc = running_corrects.double() / DATASET_SIZES[phase]\n",
    "\n",
    "            losses[phase].append(epoch_loss)\n",
    "            accs[phase].append(epoch_acc.item())\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss} Acc: {epoch_acc}')\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60}m {time_elapsed % 60}s')\n",
    "    print(f'Best val Acc: {best_acc}')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, losses, accs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model, loss, acc= train_model(model,\n",
    "                              criterion,\n",
    "                              optimizer,\n",
    "                              scheduler,\n",
    "                              num_epochs=EPOCHS)\n",
    "\n",
    "torch.save(model, f'speaker_recognition_model.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
